# YFlow

&emsp;&emsp;基于C++和cuda实现的深度学习框架YFlow，这是作者练手之作，用于总结回顾平时工作中各类算法，当然，由于局限于个人水平，目前仍有许多不足之处，目前计划是在一年内完成深度学习相关开发底层实用C++和CUDA混合开发。在实现过程中，我本人也接触学习了许多底层算法计算优化的技巧，本项目参考了Darknet框架的内容，并尝试进行突破。本项目的最终目标是成为一个基于C++和CUDA实现的高效率机器学习框架。


+ 存储

&emsp;&emsp;框架们都需要定义一个tensor类用于保存计算结果，只支持cpu的话就直接放内存就行；如果要支持gpu，就需要了解cuda和内存相关的api。很快你会发现，每次创建新tensor都cudaMalloc太慢了，所以需要像tensorflow一样运行前把所有显存都拿出来，自己做一个显存池分配，或者像pytorch一样不立即释放被垃圾回收的显存，而是缓存起来，给之后的新tensor用。那么就需要了解内存池相关的知识，例如tf就是在内部写了一个简单版本的dlmalloc。
+ 计算
&emsp;&emsp;有了用来存储的结果的数据结构，我们就需要开始写进行计算的结构，一般称为kernel。对于gpu来说，常见的深度学习用的kernel都已经由nv写在cudnn中了，所以需要学一下cudnn的使用方法，而对于cpu，两个框架好像都用的是Eigen，所以学习下Eigen就好了～当然一些特殊的kernel需要手撸cuda或者c++，所以这俩应该要熟悉，好在框架们大多没有用SIMD这个级别的优化，所以暂时不用学和cpu指令集相关的内容。有了这些kernel我们就可以尝试运行我们的框架了。那我们就要需要一个python前端，pytorch使用的是pybind11，tf是SWIG，貌似也有rfc说完转向pybind11，所以推荐学习一下pybind11。以及你会发现一个运算可能分别会有cpu，gpu两个kernel，这时你就需要在kernel之上加一层抽象，一般叫op，然后去根据配置dispatch kernel，这里可能会涉及一些c++的设计模式，也需要补充一下～有了这些我们就可以像计算器一样用我们的框架了。但是缺了深度学习框架的很重要的一部分，autodiff，也就是自动求导。tensorflow和pytorch在自动求导上的设计很不一样，tensorflow是在python那边推断出来每个op的反向运算是哪个op，然后加入图中，在c++那边就不区分前向还是反向了，pytorch则是每个op在调用的时候会在c++里面注册一个类似回调函数的反向计算。这里的图是指运算和运算结果构成的DAG。这两者的差异也就牵扯到深度学习框架的两大派，动态图和静态图，简单来说动态图就是用户一边定义运算一边运行计算，静态图是用户先定义完整个图再运行。你可以上网看看这两者的对比，然后选一个。在选了之后，如果你用的是动态图，那么就像pytorch一样写反向计算，具体的运行时就交给python就好了～也就是让python来一条一条地运行你定义的计算，python帮你做垃圾回收。选了静态图的话，你就需要写一个运行时来跑你的图，写个拓扑排序加Eigen里面的线程池就好了，tensorflow差不多就是这么弄得，垃圾回收用引用计数。截至到这里你就应该能有一个可以跑模型的框架了。通信通信算是选写的功能。主要有2个方向的：数据并行，就是好多个进程跑一个模型，让训练更快一点；模型并行，主要是让单卡放不下的模型在多卡上放下。前者你需要了解一下nccl和MPI，以及ring allreduce及其变种，他们是用于多个进程之间同步数据的；后者在单机内你研究好cuda的一些接口，多机你用grpc应该就行。

+ 项目结构

    + Random
  
        &emsp;&emsp;随机数模块，用于生成符合统计分布的随机数，基于学习理论的考虑，会依次实现当下所有随机数生成算法,实现进度如下:
        - [x] 冯·诺伊曼平方取中法及其改良算法;
        - [x] 线性同余发生器(Kobayashi混合同余发生器);
        - [ ] 梅森旋转演算法;
        - [ ] FSR发生器;
        - [ ] 组合发生器（因为效率原因，未作实现）;
  
        &emsp;&emsp;上述随机数生成算法用于生成符合均匀分布(即一个周期内所有数据出现是等概率)的随机数，再基于均匀分布生成其它分布的随机数。实现进度如下：
        - [x] 均匀分布
        - [ ] 正态分布（高斯分布）
        - [ ] 二项分布
        - [ ] 伯努利分布
        - [ ] 拉普拉斯分布
        - [ ] 泊松分布
        - [ ] 指数分布
        - [ ] 伽马分布
        - [ ] 贝塔分布
        - [ ] 狄拉克分布
    + tools

        &emsp;&emsp;深度学习常用基本初等函数的实现，目前已完成的实现有：
        - [x] 快速幂
        - [x] 卡马克快速倒数平方根算法
        - [x] 基于泰勒级数和Remez算法的自然对数函数（ln）的实现
        - [ ] 基于积分变上限函数和自适应辛普森法的自然对数函数（ln）的实现


    + Tensor

        &emsp;&emsp;实现张量数据结构，是构建所有算法模型最基本单元，实现0~3阶张量。