\hypertarget{classclass_xE2_x80_x90name}{}\doxysection{class‐name Class Reference}
\label{classclass_xE2_x80_x90name}\index{class‐name@{class‐name}}


实现张量数据结构，用于搭建神经网络  




\doxysubsection{Detailed Description}
实现张量数据结构，用于搭建神经网络 

\mbox{]} \begin{DoxyAuthor}{Author}
殉道者 
\end{DoxyAuthor}
\begin{DoxyVersion}{Version}
0.\+0.\+1 
\end{DoxyVersion}
\begin{DoxyDate}{Date}
2021年01月22日
\end{DoxyDate}
\quad{}\quad{}实现张量数据结构，用于搭建神经网络，本质是多维矩阵。需要注意的是，当进行2维以上张量的运算时，两个张量之间的 加减和二维矩阵类似。而两个高维张量的乘法可参考\+Tensorflow中matmul的源码说明来看，如果张量的dimention大于2， 实际上进行的会是batch\+\_\+mat\+\_\+mul,此时进行叉乘的是batch中的每一个切片（slice），即高维张量的运算实质还是2维张 量的运算，只不过是在更高维度的切片下，如下所示： \begin{quote}
output\mbox{[}..., \+:, \+:\mbox{]} = matrix(x\mbox{[}..., \+:, \+:\mbox{]}) $\ast$ matrix(y\mbox{[}..., \+:, \+:\mbox{]}) \end{quote}
使用tensorflow验证如下\+: \hypertarget{classclass_xE2_x80_x90name_autotoc_md1}{}\doxysubsection{f(x) = \textbackslash{}sum\+\_\+\{inti = 0\}$^\wedge$\{n\}\textbackslash{}alpha x$^\wedge$i}\label{classclass_xE2_x80_x90name_autotoc_md1}
In \mbox{[}2\mbox{]}\+: a Out\mbox{[}2\mbox{]}\+: $<$tf.\+Tensor\+: shape=(2, 3, 2), dtype=int32, numpy= array(\mbox{[}\mbox{[}\mbox{[}1, 1\mbox{]}, \mbox{[}2, 2\mbox{]}, \mbox{[}3, 3\mbox{]}\mbox{]},

\mbox{[}\mbox{[}4, 4\mbox{]}, \mbox{[}5, 5\mbox{]}, \mbox{[}6, 6\mbox{]}\mbox{]}\mbox{]})$>$

In \mbox{[}3\mbox{]}\+: c = tf.\+reshape(a,(2,2,3))

In \mbox{[}4\mbox{]}\+: c Out\mbox{[}4\mbox{]}\+: $<$tf.\+Tensor\+: shape=(2, 2, 3), dtype=int32, numpy= array(\mbox{[}\mbox{[}\mbox{[}1, 1, 2\mbox{]}, \mbox{[}2, 3, 3\mbox{]}\mbox{]},

\mbox{[}\mbox{[}4, 4, 5\mbox{]}, \mbox{[}5, 6, 6\mbox{]}\mbox{]}\mbox{]})$>$

In \mbox{[}5\mbox{]}\+: tf.\+matmul(a,c) Out\mbox{[}5\mbox{]}\+: $<$tf.\+Tensor\+: shape=(2, 3, 3), dtype=int32, numpy= array(\mbox{[}\mbox{[}\mbox{[} 3, 4, 5\mbox{]}, \mbox{[} 6, 8, 10\mbox{]}, \mbox{[} 9, 12, 15\mbox{]}\mbox{]},

\mbox{[}\mbox{[}36, 40, 44\mbox{]}, \mbox{[}45, 50, 55\mbox{]}, \hypertarget{classclass_xE2_x80_x90name_autotoc_md2}{}\doxysubsection{\mbox{[}54, 60, 66\mbox{]}\mbox{]}\mbox{]})$>$}\label{classclass_xE2_x80_x90name_autotoc_md2}
则要求高维张量相乘符合如下规则： 1.两个张量能相乘，则二者除了最后两个维度可以不相等外，其它维度必须相等； 2.两个张量能相乘，则二者的最后两个维度必须满足二维矩阵相乘的条件；

@@重点@@ 另外，因为张量乘法本质上为矩阵乘法，由于在神经网络中设计到大量的巨型矩阵的乘法，所以很有必要 优化矩阵乘法的计算逻辑，本实现使用通用矩阵乘（\+GEMM，\+General \mbox{\hyperlink{class_matrix}{Matrix}} Multiplication）优化矩阵 乘法。可参考下文\+: \href{https://jackwish.net/2019/gemm-optimization.html}{\texttt{ 通用矩阵乘（\+GEMM）优化算法}} \href{https://zhuanlan.zhihu.com/p/66958390}{\texttt{ 通用矩阵乘（\+GEMM）优化与卷积计算}} 常规的矩阵乘法计算算法的时间复杂度为\+O(n$^\wedge$3) = O(n$^\wedge$(log2(8))) 1.分块矩阵计算 2.\+Strassen 算法，时间复杂度是O(n$^\wedge$2.807) = O(n$^\wedge$(log2(7))) 3.\+Coppersmith-\/Winograd算法，时间复杂度是\+O（n$^\wedge$2.38) 

The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
C\+:/\+Users/wcjb/\+Documents/\+Code\+Space/\+YFlow/src/\mbox{\hyperlink{_tensor_8h}{Tensor.\+h}}\end{DoxyCompactItemize}
